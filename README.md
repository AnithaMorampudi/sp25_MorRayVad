# sp25_MorRayVad
# -*- coding: utf-8 -*-
"""Sp25_MorRayVad_Round5_finetune.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NmWM0-M2EFzJ3xXYMm0JkZJTcrLcu-wy

# 1: Setup + Imports
"""

#1: Install & Import Libraries
!pip install torch torchvision matplotlib

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
import matplotlib.pyplot as plt
from torch.utils.data import DataLoader, random_split
import gc
import time

from google.colab import drive
drive.mount('/content/drive')

"""# 2: Load Preprocessed CIFAR-10 Tensor Data

We load the CIFAR-10 dataset, which was **preprocessed into tensor format in Round 2.**  
This allows us to skip raw image processing and move directly to training.
"""

#2: Load Preprocessed CIFAR-10 Tensor Data
from torch.utils.data import DataLoader
import torch

# Mount Google Drive if not done yet
from google.colab import drive
drive.mount('/content/drive')

# Path to your saved tensor datasets
data_dir = '/content/drive/MyDrive/Processed_CIFAR10'

# Load the .pt files (Round 2 preprocessed tensors)
train_dataset = torch.load(f"{data_dir}/train_dataset.pt", weights_only=False)
test_dataset = torch.load(f"{data_dir}/test_dataset.pt", weights_only=False)

# Create DataLoaders
train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)

print("✅ CIFAR-10 preprocessed tensor data loaded successfully!")

"""# 3: Upgraded DeepCNN Model

This is an enhanced version of our earlier CNN:
- 3 Conv layers with BatchNorm & ReLU
- MaxPooling after conv blocks
- Fully connected layer with Dropout (p=0.5)

BatchNorm stabilizes training, Dropout prevents overfitting.
"""

#3: DeepCNN with BatchNorm & Dropout
import torch
import torch.nn as nn

class DeepCNN(nn.Module):
    def __init__(self, num_classes=10):
        super(DeepCNN, self).__init__()
        self.features = nn.Sequential(
            nn.Conv2d(3, 64, kernel_size=3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 128, kernel_size=3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),

            nn.Conv2d(128, 256, kernel_size=3, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),
        )

        self.classifier = nn.Sequential(
            nn.Flatten(),
            nn.Linear(8 * 8 * 256, 512),
            nn.ReLU(inplace=True),
            nn.Dropout(0.5),
            nn.Linear(512, num_classes)
        )

    def forward(self, x):
        x = self.features(x)
        x = self.classifier(x)
        return x

"""# 4: Training Function with Scheduler + Early Stopping

This section trains the CIFAR-10 model using:
- **SGD + Momentum (batch size = 64):** Improves convergence speed and stability (best performer in Round 4).
- **StepLR Scheduler:** Halves the learning rate every 7 epochs to fine-tune learning as training progresses.
- **Early Stopping (patience = 3):** Stops training early if validation loss doesn’t improve for 3 epochs—prevents overfitting and saves time.
"""

#4: Training Loop with Scheduler & Early Stopping
import gc
import torch.optim as optim

def train_model(model, train_loader, test_loader, num_epochs=20, lr=0.01, patience=3):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)
    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.5)

    best_loss = float('inf')
    patience_counter = 0

    train_acc_history = []
    test_acc_history = []

    for epoch in range(num_epochs):
        model.train()
        running_loss = 0.0
        correct = 0
        total = 0
        for inputs, labels in train_loader:
            inputs, labels = inputs.to(device), labels.to(device)
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            running_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

        train_acc = 100 * correct / total
        train_acc_history.append(train_acc)

        # Validation
        model.eval()
        val_loss = 0.0
        val_correct = 0
        val_total = 0
        with torch.no_grad():
            for inputs, labels in test_loader:
                inputs, labels = inputs.to(device), labels.to(device)
                outputs = model(inputs)
                loss = criterion(outputs, labels)
                val_loss += loss.item()
                _, predicted = torch.max(outputs, 1)
                val_total += labels.size(0)
                val_correct += (predicted == labels).sum().item()

        val_loss /= len(test_loader)
        val_acc = 100 * val_correct / val_total
        test_acc_history.append(val_acc)

        print(f'Epoch {epoch+1}/{num_epochs} '
              f'Train Acc: {train_acc:.2f}% '
              f'Val Acc: {val_acc:.2f}% '
              f'Val Loss: {val_loss:.4f}')

        # Early stopping
        if val_loss < best_loss:
            best_loss = val_loss
            patience_counter = 0
        else:
            patience_counter += 1
            if patience_counter >= patience:
                print("✅ Early stopping triggered!")
                break

        scheduler.step()
        gc.collect()
        torch.cuda.empty_cache()

    return train_acc_history, test_acc_history

"""###Logs per epoch:
- Train Accuracy
- Validation Accuracy
- Validation Loss

This setup balances speed, stability, and generalization—helping us reach optimal performance efficiently.
"""

# Instantiate your model
model = DeepCNN()

# Train the model
train_acc_history, test_acc_history = train_model(model, train_loader, test_loader, num_epochs=20)

"""•	Training accuracy rose quickly from ~30% to ~66% within the first 6 epochs.
	•	Validation accuracy improved from ~40% to ~70% in the same period.
	•	Progress became steadier after epoch 6, with training accuracy reaching ~90% by epoch 18.
	•	Validation accuracy plateaued around 81–82%, showing good generalization.
	•	Validation loss steadily decreased, confirming model improvements.
	•	Early stopping was triggered at epoch 18 due to no further significant gains.
The setup (momentum + scheduler + early stopping) helped achieve strong, balanced results without overfitting.

# 5: Plotting Learning Curves

We visualize training & validation accuracy across epochs to track progress.
"""

#5: Plot Learning Curves
def plot_learning_curves(train_acc_history, test_acc_history):
    plt.figure(figsize=(8, 5))
    plt.plot(train_acc_history, label='Train Accuracy')
    plt.plot(test_acc_history, label='Validation Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy (%)')
    plt.title('Training vs. Validation Accuracy')
    plt.legend()
    plt.grid()
    plt.show()

# Plot accuracy curves
plot_learning_curves(train_acc_history, test_acc_history)

"""- The blue curve (Train Accuracy) steadily increases across epochs, starting at ~30% and reaching close to 90% by epoch 17.
- The orange curve (Validation Accuracy) also rises quickly at first (40% ➔ ~77% in ~7 epochs) and plateaus around 82% toward the end.

**This tells us:**

- **Strong Learning**: Both train and validation accuracy improve fast in early epochs (good learning progress).
-	**Good Generalization**: Validation accuracy tracks close to training accuracy up to ~epoch 8–10.
- **Mild Overfitting**: After ~epoch 10, the train accuracy keeps rising, but the validation curve levels off—hinting at slight overfitting as the gap between them grows.
- **Early Stopping Worked Well**: Training likely stopped around 18 epochs because the validation improvements became minor—showing early stopping was effective.

# 6: Confusion Matrix + Error Analysis

We plot a confusion matrix to visualize **which classes were predicted correctly** vs. misclassified.  
Helps identify tricky classes (e.g., Car vs Truck).
"""

#6: Confusion Matrix & Misclassified Samples
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import numpy as np

def plot_confusion_matrix(model, test_loader, class_names):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.eval()
    all_preds = []
    all_labels = []

    with torch.no_grad():
        for inputs, labels in test_loader:
            inputs = inputs.to(device)
            outputs = model(inputs)
            _, preds = torch.max(outputs, 1)
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.numpy())

    cm = confusion_matrix(all_labels, all_preds)
    fig, ax = plt.subplots(figsize=(8, 8))
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)
    disp.plot(cmap=plt.cm.Blues, ax=ax)
    plt.title("Confusion Matrix")
    plt.show()

# CIFAR-10 class names
class_names = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']

plot_confusion_matrix(model, test_loader, class_names)

"""**Brief interpretation of confusion matrix:**
- The model classifies ‘ship’, ‘truck’, ‘car’, and ‘frog’ with high accuracy.
- Moderate confusion exists between ‘cat’ vs. ‘dog’ and ‘bird’ vs. ‘deer’, which is common due to visual similarities.
- Some misclassifications are noticeable between ‘truck’ and ‘car’ and ‘ship’ and ‘plane’, likely due to overlapping features.
- Overall, most diagonal cells are strong (700–900+), showing the model generalizes well across all 10 classes.

# 7: Visualising Misclassified Images

We display misclassified images **chosen dynamically** based on user input.  
Each image shows:
- The predicted label
- The true label

Useful for inspecting model errors.
"""

# 7: Showing Misclassified Images if user wants to see
def show_misclassified(model, test_loader, class_names, num_images=5):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.eval()
    misclassified = []

    with torch.no_grad():
        for inputs, labels in test_loader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            _, preds = torch.max(outputs, 1)
            for img, pred, label in zip(inputs, preds, labels):
                if pred != label:
                    misclassified.append((img.cpu(), pred.cpu(), label.cpu()))

    # Capping the number if user asks for too many
    total_misclassified = len(misclassified)
    if total_misclassified == 0:
        print("✅ No misclassified images found!")
        return

    if num_images > total_misclassified:
        print(f"⚠️ Only {total_misclassified} misclassified images available. Showing all.")
        num_images = total_misclassified

    plt.figure(figsize=(3 * num_images, 3))
    for idx, (img, pred, label) in enumerate(misclassified[:num_images]):
        plt.subplot(1, num_images, idx + 1)
        img = img.permute(1, 2, 0) * 0.5 + 0.5  # unnormalize
        plt.imshow(img)
        plt.title(f'Pred: {class_names[pred]}\nTrue: {class_names[label]}')
        plt.axis('off')
    plt.show()

# asking user for input
try:
    num_images = int(input("How many misclassified images would you like to display? "))
    show_misclassified(model, test_loader, class_names, num_images=num_images)
except ValueError:
    print("Error❌ Please enter a valid integer.")
